{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 1:  Information Retrieval Basics\n",
    "\n",
    "### 100 points [7% of your final grade]\n",
    "\n",
    "### Due: \n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via eCampus (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is collected from Quizlet (https://quizlet.com), a website where users can generated their own flashcards. Each flashcard generated by a user is made up of an entity on the front and a definition describing or explaining the entity correspondingly on the back. We treat entities on each flashcard's front as the queries and the definitions on the back of flashcards as the documents. Definitions (documents) are relevant to an entity (query) if the definitions are from the back of the entity's flashcard; otherwise definitions are not relevant. **In this homework, queries and entities are interchangeable as well as documents and definitions.**\n",
    "\n",
    "The format of the dataset is like this:\n",
    "\n",
    "**query \\t document id \\t document**\n",
    "\n",
    "Examples:\n",
    "\n",
    "decision tree\t\\t 27946 \\t\tshow complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\n",
    "\n",
    "where \"decision tree\" is the entity in the front of a flashcard and \"show complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\" is the definition on the flashcard's back and \"27946\" is the id of the definition. Naturally, this document is relevant to the query.\n",
    "\n",
    "false positive rate\t\\t 686\t\\t fall-out; probability of a false alarm\n",
    "\n",
    "where document 686 is not relevant to query \"decision tree\" because the entity of \"fall-out; probability of a false alarm\" is \"false positive rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parsing (20 points)\n",
    "\n",
    "First, you should tokenize documents (definitions) using **whitespaces and punctuations as delimiters**. Your parser needs to also provide the following three pre-processing options:\n",
    "* Remove stop words: use nltk stop words list (from nltk.corpus import stopwords)\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "* Remove any other strings that you think are less informative or nosiy.\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration options\n",
    "remove_stopwords = True  # or false\n",
    "use_stemming = True # or false\n",
    "remove_otherNoise = True # or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15742\n",
      "15602\n",
      "9722\n",
      "9458\n"
     ]
    }
   ],
   "source": [
    "# Your parser function here. It will take the three option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "f = open(\"homework_1_data.txt\", encoding='UTF-8')             \n",
    "line = f.readline()             \n",
    "strings = ''\n",
    "while line:\n",
    "    line_list = line.split(\"\\t\")\n",
    "    string = re.sub(\"[^A-Z^a-z^0-9^ ]\", \" \", line_list[2])\n",
    "    strings = strings + string\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "\n",
    "words = nltk.word_tokenize(strings)\n",
    "words_freq_dist = nltk.FreqDist(words)\n",
    "print(len(words_freq_dist))\n",
    "\n",
    "if remove_stopwords and not use_stemming and not remove_otherNoise:\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    filtered_words_freq_dist = nltk.FreqDist(filtered_words)\n",
    "    print(len(filtered_words_freq_dist))\n",
    "\n",
    "if remove_stopwords and use_stemming and not remove_otherNoise:\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    filtered_words_freq_dist = nltk.FreqDist(filtered_words)\n",
    "    print(len(filtered_words_freq_dist))\n",
    "    stemmer = PorterStemmer()\n",
    "    singles = [stemmer.stem(plural) for plural in filtered_words]\n",
    "    singles_freq_dist = nltk.FreqDist(singles)\n",
    "    print(len(singles_freq_dist))\n",
    "\n",
    "# remove other noise: separate digits\n",
    "if remove_stopwords and use_stemming and remove_otherNoise:\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    filtered_words_freq_dist = nltk.FreqDist(filtered_words)\n",
    "    print(len(filtered_words_freq_dist))\n",
    "    stemmer = PorterStemmer()\n",
    "    singles = [stemmer.stem(plural) for plural in filtered_words]\n",
    "    singles_freq_dist = nltk.FreqDist(singles)\n",
    "    print(len(singles_freq_dist))\n",
    "    singles_no_digits = []\n",
    "    for x in singles:\n",
    "        if not x.isdigit():\n",
    "            singles_no_digits.append(x)\n",
    "    singles_no_digits_freq_dist = nltk.FreqDist(singles_no_digits)\n",
    "    print(len(singles_no_digits_freq_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* None of pre-processing options      = 15742\n",
    "* remove stop words       = 15602\n",
    "* remove stop words + stemming       = 9722\n",
    "* remove stop words + stemming  + remove other noise     = 9458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Boolean Retrieval (30 points)\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **relational model**, your search engine should treat it as **relational** AND **model**.\n",
    "\n",
    "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
    "* relational database\n",
    "* garbage collection\n",
    "* retrieval model\n",
    "\n",
    "Please use the following format to present your results:\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the index here\n",
    "# add cells as needed to organize your code\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "f = open(\"homework_1_data.txt\", encoding='UTF-8')             \n",
    "f2 = open(\"definition.txt\", 'w', encoding='UTF-8')\n",
    "f3 = open(\"entity.txt\", 'w', encoding='UTF-8')\n",
    "f4 = open(\"original_definition.txt\", 'w', encoding='UTF-8')\n",
    "line = f.readline()             \n",
    "strings = ''\n",
    "while line:\n",
    "    line_list = line.split(\"\\t\")\n",
    "    string = re.sub(\"[^A-Z^a-z^0-9^ ]\", \" \", line_list[2])\n",
    "    strings = strings + string\n",
    "\n",
    "    line_words = nltk.word_tokenize(string)\n",
    "    filtered_line_words = [line_word for line_word in line_words if line_word not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    line_singles = [stemmer.stem(line_plural) for line_plural in filtered_line_words]\n",
    "    line_singles_no_digits = []\n",
    "    for x in line_singles:\n",
    "        if not x.isdigit():\n",
    "            line_singles_no_digits.append(x)\n",
    "    # list -> string\n",
    "    list_to_string = \" \".join(line_singles_no_digits)\n",
    "    f2.write(list_to_string+\"\\n\")\n",
    "\n",
    "    f3.write(line_list[0] + \"\\n\")\n",
    "    f4.write(line_list[2])\n",
    "\n",
    "    line = f.readline()\n",
    "\n",
    "f.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()\n",
    "\n",
    "words = nltk.word_tokenize(strings)\n",
    "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in filtered_words]\n",
    "\n",
    "singles_no_digits = []\n",
    "for x in singles:\n",
    "    if not x.isdigit():\n",
    "        singles_no_digits.append(x)\n",
    "\n",
    "# construct index\n",
    "f = open(\"definition.txt\", encoding='UTF-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "singles_no_digits_freq_dist = nltk.FreqDist(singles_no_digits)\n",
    "sorted_list = sorted(singles_no_digits_freq_dist)\n",
    "invert_index = dict()\n",
    "for b in sorted_list:\n",
    "    temp = []\n",
    "    ID = 0\n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        if b in split_line:\n",
    "            temp.append(ID)\n",
    "        ID = ID + 1\n",
    "    invert_index[b] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "def boolean_query(argus):\n",
    "    query = argus\n",
    "    query_tokens = nltk.word_tokenize(query)\n",
    "    filtered_query_tokens = [query_token for query_token in query_tokens if\n",
    "                             query_token not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    query_singles = [stemmer.stem(query_plural) for query_plural in filtered_query_tokens]\n",
    "    query_singles_no_digits = []\n",
    "    for x in query_singles:\n",
    "        if not x.isdigit():\n",
    "            query_singles_no_digits.append(x)\n",
    "\n",
    "    query_word1 = query_singles_no_digits[0]\n",
    "    query_word2 = query_singles_no_digits[1]\n",
    "\n",
    "    no_order_merge_list = list(set(invert_index[query_word1]).intersection(set(invert_index[query_word2])))\n",
    "    merge_list = sorted(no_order_merge_list)\n",
    "\n",
    "    # print\n",
    "    f = open(\"entity.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    f = open(\"original_definition.txt\", encoding='UTF-8')\n",
    "    original_definition_lines = f.readlines()\n",
    "    f.close()\n",
    "    print(\"query: \" + query)\n",
    "    for i in range(5):        \n",
    "        print(\"result {}:\".format(i + 1))\n",
    "        print(\"entity: \" + lines[merge_list[i]], end=\"\")\n",
    "        print(\"definition id: {}\".format(merge_list[i]))\n",
    "        print(\"definition: \" + original_definition_lines[merge_list[i]], end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: relational database\n",
      "result 1:\n",
      "entity: database management system\n",
      "definition id: 654\n",
      "definition: dbms allows users to create, read, update, and delete structured data in a relational database. managers send requests to dbms and the dbms performs manipulation of the data. can retrieve information from using sql or qbe (query by example).   relational database management system: allows users to create, read, update, and delete data in a relational database.  pros: increased flexibility, inc scalability and performance, reduced info redundancy, inc info integrity/quality, increased info security.\n",
      "result 2:\n",
      "entity: database management system\n",
      "definition id: 657\n",
      "definition: general hospital utilizes various related files that include clinical and financial data to generate reports such as ms drg case mix reports. what application would be most effective for this activity  desktop publishing  word processing database management system command interpreter\n",
      "result 3:\n",
      "entity: database management system\n",
      "definition id: 682\n",
      "definition: database management system software that controls how related collections od data are stored.\n",
      "result 4:\n",
      "entity: relational model\n",
      "definition id: 741\n",
      "definition: developed by ef codd of ibm in 19070, the relational model is based on mathematical set theory and represents data as independent relations. each relation (table) is conceptually represented as a two dimensional structure of intersecting rows and columns. the relations are related to each other through the sharing of common entity characteristics (values in columns). -to use an analogy, it produced an &\"automatic transmission&\" database to replace the &\"standard transmission&\" databases that preceded it. -describes a precise set of data manipulation constructs based on advanced mathematical concepts.\n",
      "result 5:\n",
      "entity: relational model\n",
      "definition id: 750\n",
      "definition: all relational database dbms products are built on this model -e.f. codd applied the concepts of a branch of math called relational algebra to the problem of databases\n",
      "\n",
      "\n",
      "query: garbage collection\n",
      "result 1:\n",
      "entity: garbage collector\n",
      "definition id: 4150\n",
      "definition: the part of the operating system that performs garbage collection.\n",
      "result 2:\n",
      "entity: memory safety\n",
      "definition id: 13115\n",
      "definition: memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result 3:\n",
      "entity: garbage collection\n",
      "definition id: 21550\n",
      "definition: automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "result 4:\n",
      "entity: garbage collection\n",
      "definition id: 21553\n",
      "definition: garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result 5:\n",
      "entity: garbage collection\n",
      "definition id: 21554\n",
      "definition: there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "\n",
      "\n",
      "query: retrieval model\n",
      "result 1:\n",
      "entity: data model\n",
      "definition id: 6983\n",
      "definition: - a collection of concepts that can be used to describe the structure of a database - describes the structure - dbms is based on a data model   structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)  types of data models: - high-level (conceptual i.e. er) - low level (physical i.e. xml) - implementation (representational) combines conceptual and physical i.e. relational model - nosql data models i.e. column, key-value, document stores\n",
      "result 2:\n",
      "entity: data model\n",
      "definition id: 7031\n",
      "definition: \\ structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)\n",
      "result 3:\n",
      "entity: query language\n",
      "definition id: 9085\n",
      "definition: used to update and retrieve data that is stored in a data model\n",
      "result 4:\n",
      "entity: physical design\n",
      "definition id: 10327\n",
      "definition: translate logical model into technical specifications for storing/retrieving data, store data to achieve efficiency and quality\n",
      "result 5:\n",
      "entity: physical design\n",
      "definition id: 10335\n",
      "definition: translate logical model into technical specifications for storing / retrieving data. store data to achieve efficiency and quality\n"
     ]
    }
   ],
   "source": [
    "# search for the input using your index and print out ids of matching documents.\n",
    "boolean_query(\"relational database\")\n",
    "print(\"\\n\")\n",
    "boolean_query(\"garbage collection\")\n",
    "print(\"\\n\")\n",
    "boolean_query(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Could your boolean search engine find relevant documents for these queries? What is the impact of the three pre-processing options? Do they improve your search quality?\n",
    "\n",
    "Yes!\n",
    "\n",
    "The three pre-processing options help me find relevant documents for these queries. Not just precisely match the words in the documents can i find the expected results. If the documents contain the meaning in the query, i can also find them.\n",
    "\n",
    "Definitely, they improve my search quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Ranking Documents (50 points) \n",
    "\n",
    "In this part, your job is to rank the documents that have been retrieved by the Boolean Retrieval component in Part 2, according to their relevance with each query.\n",
    "\n",
    "### A: Ranking with simple sums of TF-IDF scores (15 points) \n",
    "For a multi-word query, we rank documents by a simple sum of the TF-IDF scores for the query terms in the document.\n",
    "TF is the log-weighted term frequency $1+log(tf)$; and IDF is the log-weighted inverse document frequency $log(\\frac{N}{df})$\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 results plus the TF-IDF sum score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# hint: you could first call boolean retrieval function in part 2 to find possible relevant documents, \n",
    "# and then rank these documents in this part. Hence, you don't need to rank all documents.\n",
    "import math\n",
    "\n",
    "def boolean_query_sum_tf_idf(argus):\n",
    "    query = argus\n",
    "    query_tokens = nltk.word_tokenize(query)\n",
    "    filtered_query_tokens = [query_token for query_token in query_tokens if\n",
    "                             query_token not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    query_singles = [stemmer.stem(query_plural) for query_plural in filtered_query_tokens]\n",
    "    query_singles_no_digits = []\n",
    "    for x in query_singles:\n",
    "        if not x.isdigit():\n",
    "            query_singles_no_digits.append(x)\n",
    "\n",
    "    query_word1 = query_singles_no_digits[0]\n",
    "    query_word2 = query_singles_no_digits[1]\n",
    "\n",
    "    list1 = invert_index[query_word1]\n",
    "    list2 = invert_index[query_word2]\n",
    "    merge_list = [new for new in list1 if new in list2]\n",
    "\n",
    "    f = open(\"definition.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    sum_wtd_index = dict()\n",
    "\n",
    "    for i in range(len(merge_list)):\n",
    "        query_word1_wtd = (1 + math.log10(lines[merge_list[i]].count(query_word1))) * math.log10(\n",
    "            30917 / len(invert_index[query_word1]))\n",
    "        query_word2_wtd = (1 + math.log10(lines[merge_list[i]].count(query_word2))) * math.log10(\n",
    "            30917 / len(invert_index[query_word2]))\n",
    "        sum_wtd = query_word1_wtd + query_word2_wtd\n",
    "        sum_wtd_index[merge_list[i]] = sum_wtd\n",
    "\n",
    "    sorted_sum_wtd_index = dict(sorted(sum_wtd_index.items(), key=lambda d: d[1], reverse=True))\n",
    "    sorted_keys_list = list(sorted_sum_wtd_index.keys())\n",
    "\n",
    "    # print\n",
    "    f = open(\"entity.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    f = open(\"original_definition.txt\", encoding='UTF-8')\n",
    "    original_definition_lines = f.readlines()\n",
    "    f.close()\n",
    "    print(\"query: \" + query)\n",
    "    for i in range(5):\n",
    "        print(\"result {}:\".format(i + 1))\n",
    "        print(\"score: {}\".format(sorted_sum_wtd_index[sorted_keys_list[i]]))\n",
    "        print(\"entity: \" + lines[sorted_keys_list[i]], end=\"\")\n",
    "        print(\"definition id: {}\".format(sorted_keys_list[i]))\n",
    "        print(\"definition: \" + original_definition_lines[sorted_keys_list[i]], end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: relational database\n",
      "result 1:\n",
      "score: 4.71733880527531\n",
      "entity: relational algebra\n",
      "definition id: 7156\n",
      "definition: - a theoretical language with operations that work on one or more relations to define another relation without changing the original relation(s)  - relation-at-a-time (or set) language in which all tuples, possibly from several relations, are manipulated in one statement without looping  relational algebra, first created by edgar f. codd while at ibm, is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it.  the main application of relational algebra is providing a theoretical foundation for relational databases, particularly query languages for such databases, chief among which is sql.\n",
      "result 2:\n",
      "score: 4.357658330802902\n",
      "entity: relational database\n",
      "definition id: 28378\n",
      "definition: a type of database system where data is stored in  tables related by common fields. a relational database is the most common type of database used with a personal computer. similar data is held in a table (e.g. students, courses, books, instructors) and tables are related through a common field (a field that is in more than one table) microsoft access and corel paradox are both relational database management systems.\n",
      "result 3:\n",
      "score: 4.238364518320238\n",
      "entity: relational database\n",
      "definition id: 28254\n",
      "definition: finite set of relations​. each relation consists of a schema and an instance​. database schema = set of relation schemas constraints among relations (inter-relational constraints)​. database instance = set of (corresponding) relation instances\n",
      "result 4:\n",
      "score: 4.122275123103649\n",
      "entity: relational model\n",
      "definition id: 741\n",
      "definition: developed by ef codd of ibm in 19070, the relational model is based on mathematical set theory and represents data as independent relations. each relation (table) is conceptually represented as a two dimensional structure of intersecting rows and columns. the relations are related to each other through the sharing of common entity characteristics (values in columns). -to use an analogy, it produced an &\"automatic transmission&\" database to replace the &\"standard transmission&\" databases that preceded it. -describes a precise set of data manipulation constructs based on advanced mathematical concepts.\n",
      "result 5:\n",
      "score: 4.017820665941266\n",
      "entity: database management system\n",
      "definition id: 654\n",
      "definition: dbms allows users to create, read, update, and delete structured data in a relational database. managers send requests to dbms and the dbms performs manipulation of the data. can retrieve information from using sql or qbe (query by example).   relational database management system: allows users to create, read, update, and delete data in a relational database.  pros: increased flexibility, inc scalability and performance, reduced info redundancy, inc info integrity/quality, increased info security.\n",
      "\n",
      "\n",
      "query: garbage collection\n",
      "result 1:\n",
      "score: 7.185876112272187\n",
      "entity: garbage collection\n",
      "definition id: 21553\n",
      "definition: garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result 2:\n",
      "score: 6.3292301409376925\n",
      "entity: garbage collection\n",
      "definition id: 21550\n",
      "definition: automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "result 3:\n",
      "score: 6.3292301409376925\n",
      "entity: garbage collection\n",
      "definition id: 21554\n",
      "definition: there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "result 4:\n",
      "score: 5.520628775347339\n",
      "entity: garbage collection\n",
      "definition id: 21559\n",
      "definition: garbage collection is a feature that automatically deletes unused memory that is no longer being referenced. you cannot force it but you can request it using system.gc() but you should never use that because it slows down your program heavily. once the system thinks an object it ready for collection, it will call finalize() on it which does final cleanup and prepares it do be collected\n",
      "result 5:\n",
      "score: 4.864784180250639\n",
      "entity: garbage collector\n",
      "definition id: 4150\n",
      "definition: the part of the operating system that performs garbage collection.\n",
      "\n",
      "\n",
      "query: retrieval model\n",
      "result 1:\n",
      "score: 4.292304706110251\n",
      "entity: data model\n",
      "definition id: 6983\n",
      "definition: - a collection of concepts that can be used to describe the structure of a database - describes the structure - dbms is based on a data model   structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)  types of data models: - high-level (conceptual i.e. er) - low level (physical i.e. xml) - implementation (representational) combines conceptual and physical i.e. relational model - nosql data models i.e. column, key-value, document stores\n",
      "result 2:\n",
      "score: 3.3370417172064877\n",
      "entity: data model\n",
      "definition id: 7031\n",
      "definition: \\ structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)\n",
      "result 3:\n",
      "score: 3.3370417172064877\n",
      "entity: query language\n",
      "definition id: 9085\n",
      "definition: used to update and retrieve data that is stored in a data model\n",
      "result 4:\n",
      "score: 3.3370417172064877\n",
      "entity: physical design\n",
      "definition id: 10327\n",
      "definition: translate logical model into technical specifications for storing/retrieving data, store data to achieve efficiency and quality\n",
      "result 5:\n",
      "score: 3.3370417172064877\n",
      "entity: physical design\n",
      "definition id: 10335\n",
      "definition: translate logical model into technical specifications for storing / retrieving data. store data to achieve efficiency and quality\n"
     ]
    }
   ],
   "source": [
    "boolean_query_sum_tf_idf(\"relational database\")\n",
    "print(\"\\n\")\n",
    "boolean_query_sum_tf_idf(\"garbage collection\")\n",
    "print(\"\\n\")\n",
    "boolean_query_sum_tf_idf(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Ranking with vector space model with TF-IDF (15 points) \n",
    "\n",
    "**Cosine:** You should use cosine as your scoring function. \n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TF-IDF scores as introduced in A. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 documents plus the cosine score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "\n",
    "def boolean_query_vsm_tf_idf(argus):\n",
    "    query = argus\n",
    "    query_tokens = nltk.word_tokenize(query)\n",
    "    filtered_query_tokens = [query_token for query_token in query_tokens if\n",
    "                             query_token not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    query_singles = [stemmer.stem(query_plural) for query_plural in filtered_query_tokens]\n",
    "    query_singles_no_digits = []\n",
    "    for x in query_singles:\n",
    "        if not x.isdigit():\n",
    "            query_singles_no_digits.append(x)\n",
    "\n",
    "    query_word1 = query_singles_no_digits[0]\n",
    "    query_word2 = query_singles_no_digits[1]\n",
    "\n",
    "    list1 = invert_index[query_word1]\n",
    "    list2 = invert_index[query_word2]\n",
    "    merge_list = [new for new in list1 if new in list2]\n",
    "\n",
    "    f = open(\"definition.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    docID_score_dict = dict()\n",
    "\n",
    "    for i in range(len(merge_list)):\n",
    "        # each line: string -> list\n",
    "        line_list = lines[merge_list[i]].split()\n",
    "        # each line: -> dict\n",
    "        line_list_dict = nltk.FreqDist(line_list)\n",
    "        # dict values -> vector\n",
    "        dict_values_list = list(line_list_dict.values())\n",
    "        doc_vector = np.array(dict_values_list)\n",
    "        doc_vector = doc_vector / np.linalg.norm(doc_vector)\n",
    "        # query -> dict\n",
    "        query_dict = dict([(k, 0) for k in line_list_dict.keys()])\n",
    "        query_dict[query_word1] = 1\n",
    "        query_dict[query_word2] = 1\n",
    "        # dict values -> vector\n",
    "        dict_values_list = list(query_dict.values())        \n",
    "        query_vector = np.array(dict_values_list)\n",
    "        query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "        # compute score\n",
    "        docID_score_dict[merge_list[i]] = query_vector.dot(doc_vector)\n",
    "\n",
    "    sorted_docID_score_dict = dict(sorted(docID_score_dict.items(), key=lambda d: d[1], reverse=True))\n",
    "    sorted_keys_list = list(sorted_docID_score_dict.keys())\n",
    "\n",
    "    # print\n",
    "    f = open(\"entity.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    f = open(\"original_definition.txt\", encoding='UTF-8')\n",
    "    original_definition_lines = f.readlines()\n",
    "    f.close()\n",
    "    print(\"query: \" + query)\n",
    "    for i in range(5):\n",
    "        print(\"result {}:\".format(i + 1))\n",
    "        print(\"score: {}\".format(sorted_docID_score_dict[sorted_keys_list[i]]))\n",
    "        print(\"entity: \" + lines[sorted_keys_list[i]], end=\"\")\n",
    "        print(\"definition id: {}\".format(sorted_keys_list[i]))\n",
    "        print(\"definition: \" + original_definition_lines[sorted_keys_list[i]], end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: relational database\n",
      "result 1:\n",
      "score: 0.7878385971583353\n",
      "entity: relational database\n",
      "definition id: 28234\n",
      "definition: a database that is modeled using the relational database model a collection of related relations within which each relation has a unique name\n",
      "result 2:\n",
      "score: 0.7499999999999998\n",
      "entity: relational database\n",
      "definition id: 28205\n",
      "definition: a database built using the relational database model\n",
      "result 3:\n",
      "score: 0.7302967433402214\n",
      "entity: relational database\n",
      "definition id: 28312\n",
      "definition: a collection of related relations in which each relation has a unique name  operational/transactional databases\n",
      "result 4:\n",
      "score: 0.7071067811865475\n",
      "entity: relational model\n",
      "definition id: 771\n",
      "definition: a database is a collection of relations or tables.\n",
      "result 5:\n",
      "score: 0.7071067811865475\n",
      "entity: database schema\n",
      "definition id: 19673\n",
      "definition: set of schemas for the relations of a database\n",
      "\n",
      "\n",
      "query: garbage collection\n",
      "result 1:\n",
      "score: 0.5773502691896258\n",
      "entity: garbage collector\n",
      "definition id: 4150\n",
      "definition: the part of the operating system that performs garbage collection.\n",
      "result 2:\n",
      "score: 0.5252257314388902\n",
      "entity: garbage collection\n",
      "definition id: 21553\n",
      "definition: garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result 3:\n",
      "score: 0.5163977794943222\n",
      "entity: garbage collection\n",
      "definition id: 21554\n",
      "definition: there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "result 4:\n",
      "score: 0.4472135954999579\n",
      "entity: memory safety\n",
      "definition id: 13115\n",
      "definition: memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result 5:\n",
      "score: 0.4364357804719847\n",
      "entity: garbage collection\n",
      "definition id: 21559\n",
      "definition: garbage collection is a feature that automatically deletes unused memory that is no longer being referenced. you cannot force it but you can request it using system.gc() but you should never use that because it slows down your program heavily. once the system thinks an object it ready for collection, it will call finalize() on it which does final cleanup and prepares it do be collected\n",
      "\n",
      "\n",
      "query: retrieval model\n",
      "result 1:\n",
      "score: 0.4714045207910316\n",
      "entity: query language\n",
      "definition id: 9085\n",
      "definition: used to update and retrieve data that is stored in a data model\n",
      "result 2:\n",
      "score: 0.4714045207910316\n",
      "entity: online analytical processing\n",
      "definition id: 19727\n",
      "definition: tools for retrieving, processing, and modelling data from the data warehouse\n",
      "result 3:\n",
      "score: 0.4714045207910316\n",
      "entity: online analytical processing\n",
      "definition id: 19731\n",
      "definition: enable retrieving, processing, and modeling data from the data warehouse\n",
      "result 4:\n",
      "score: 0.4472135954999579\n",
      "entity: online analytical processing\n",
      "definition id: 19708\n",
      "definition: olap - tools for retrieving, processing, and modeling data from the data warehouse\n",
      "result 5:\n",
      "score: 0.42640143271122083\n",
      "entity: information processing\n",
      "definition id: 17315\n",
      "definition: taking in information, figuring out whats important, storing and retrieving processes  computer model= same software\n"
     ]
    }
   ],
   "source": [
    "boolean_query_vsm_tf_idf(\"relational database\")\n",
    "print(\"\\n\")\n",
    "boolean_query_vsm_tf_idf(\"garbage collection\")\n",
    "print(\"\\n\")\n",
    "boolean_query_vsm_tf_idf(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: Ranking with BM25 (20 points) \n",
    "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 documents plus the BM25 score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def boolean_query_BM25(argus):\n",
    "    query = argus\n",
    "    query_tokens = nltk.word_tokenize(query)\n",
    "    filtered_query_tokens = [query_token for query_token in query_tokens if\n",
    "                             query_token not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    query_singles = [stemmer.stem(query_plural) for query_plural in filtered_query_tokens]\n",
    "    query_singles_no_digits = []\n",
    "    for x in query_singles:\n",
    "        if not x.isdigit():\n",
    "            query_singles_no_digits.append(x)\n",
    "\n",
    "    query_word1 = query_singles_no_digits[0]\n",
    "    query_word2 = query_singles_no_digits[1]\n",
    "\n",
    "    list1 = invert_index[query_word1]\n",
    "    list2 = invert_index[query_word2]\n",
    "    merge_list = [new for new in list1 if new in list2]\n",
    "\n",
    "    # compute avgdl\n",
    "    f = open(\"definition.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    num_words = 0\n",
    "    for i in range(30917):\n",
    "        num_words = num_words + len(lines[i].split())\n",
    "    avgdl = num_words / 30917\n",
    "\n",
    "    score_BM25_dict = dict()\n",
    "\n",
    "    f = open(\"definition.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(merge_list)):\n",
    "        idf_query_word1 = math.log10((30917 - len(invert_index[query_word1]) + 0.5) / (\n",
    "                (len(invert_index[query_word1])) + 0.5))\n",
    "        idf_query_word2 = math.log10((30917 - len(invert_index[query_word2]) + 0.5) / (\n",
    "                (len(invert_index[query_word2])) + 0.5))\n",
    "        score_BM25_part1 = idf_query_word1 * (lines[merge_list[i]].count(query_word1) * (1.2 + 1)) / (\n",
    "                    lines[merge_list[i]].count(query_word1) + 1.2 * (\n",
    "                        1 - 0.75 + 0.75 * len(lines[merge_list[i]].split()) / avgdl))\n",
    "        score_BM25_part2 = idf_query_word2 * (lines[merge_list[i]].count(query_word2) * (1.2 + 1)) / (\n",
    "                    lines[merge_list[i]].count(query_word2) + 1.2 * (\n",
    "                        1 - 0.75 + 0.75 * len(lines[merge_list[i]].split()) / avgdl))\n",
    "        score_BM25 = score_BM25_part1 + score_BM25_part2\n",
    "        score_BM25_dict[merge_list[i]] = score_BM25\n",
    "    sorted_score_BM25_dict = dict(sorted(score_BM25_dict.items(), key=lambda d: d[1], reverse=True))\n",
    "    sorted_keys_list = list(sorted_score_BM25_dict.keys())\n",
    "\n",
    "    # print\n",
    "    f = open(\"entity.txt\", encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    f = open(\"original_definition.txt\", encoding='UTF-8')\n",
    "    original_definition_lines = f.readlines()\n",
    "    f.close()\n",
    "    print(\"query: \" + query)\n",
    "    for i in range(5):\n",
    "        print(\"result {}:\".format(i + 1))\n",
    "        print(\"score: {}\".format(sorted_score_BM25_dict[sorted_keys_list[i]]))\n",
    "        print(\"entity: \" + lines[sorted_keys_list[i]], end=\"\")\n",
    "        print(\"definition id: {}\".format(sorted_keys_list[i]))\n",
    "        print(\"definition: \" + original_definition_lines[sorted_keys_list[i]], end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: relational database\n",
      "result 1:\n",
      "score: 4.068414906597672\n",
      "entity: relational database\n",
      "definition id: 28234\n",
      "definition: a database that is modeled using the relational database model a collection of related relations within which each relation has a unique name\n",
      "result 2:\n",
      "score: 3.7876375911531026\n",
      "entity: relational database\n",
      "definition id: 28205\n",
      "definition: a database built using the relational database model\n",
      "result 3:\n",
      "score: 3.7750544210039623\n",
      "entity: relational database\n",
      "definition id: 28312\n",
      "definition: a collection of related relations in which each relation has a unique name  operational/transactional databases\n",
      "result 4:\n",
      "score: 3.7384378453329443\n",
      "entity: relational model\n",
      "definition id: 795\n",
      "definition: - database is a collection of relations - each relation has attributes and a collection of tuples\n",
      "result 5:\n",
      "score: 3.682438274725153\n",
      "entity: relational model\n",
      "definition id: 771\n",
      "definition: a database is a collection of relations or tables.\n",
      "\n",
      "\n",
      "query: garbage collection\n",
      "result 1:\n",
      "score: 6.732739208125666\n",
      "entity: garbage collection\n",
      "definition id: 21553\n",
      "definition: garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result 2:\n",
      "score: 6.06030636755113\n",
      "entity: garbage collector\n",
      "definition id: 4150\n",
      "definition: the part of the operating system that performs garbage collection.\n",
      "result 3:\n",
      "score: 5.3672000515009675\n",
      "entity: garbage collection\n",
      "definition id: 21554\n",
      "definition: there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "result 4:\n",
      "score: 5.171676405790655\n",
      "entity: memory safety\n",
      "definition id: 13115\n",
      "definition: memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result 5:\n",
      "score: 4.6592796052419505\n",
      "entity: garbage collection\n",
      "definition id: 21576\n",
      "definition: a class instance is explicitly created by the java code and after use it is automatically destroyed by garbage collection for memory management.\n",
      "\n",
      "\n",
      "query: retrieval model\n",
      "result 1:\n",
      "score: 3.9895404242539416\n",
      "entity: query language\n",
      "definition id: 9085\n",
      "definition: used to update and retrieve data that is stored in a data model\n",
      "result 2:\n",
      "score: 3.9895404242539416\n",
      "entity: online analytical processing\n",
      "definition id: 19727\n",
      "definition: tools for retrieving, processing, and modelling data from the data warehouse\n",
      "result 3:\n",
      "score: 3.9895404242539416\n",
      "entity: online analytical processing\n",
      "definition id: 19731\n",
      "definition: enable retrieving, processing, and modeling data from the data warehouse\n",
      "result 4:\n",
      "score: 3.8317221108631405\n",
      "entity: online analytical processing\n",
      "definition id: 19708\n",
      "definition: olap - tools for retrieving, processing, and modeling data from the data warehouse\n",
      "result 5:\n",
      "score: 3.425235521785278\n",
      "entity: information processing\n",
      "definition id: 17315\n",
      "definition: taking in information, figuring out whats important, storing and retrieving processes  computer model= same software\n"
     ]
    }
   ],
   "source": [
    "boolean_query_BM25(\"relational database\")\n",
    "print(\"\\n\")\n",
    "boolean_query_BM25(\"garbage collection\")\n",
    "print(\"\\n\")\n",
    "boolean_query_BM25(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Briefly discuss the differences you see between the three methods. Is there one you prefer?\n",
    "\n",
    "About the query \"relational database\", the number of top 5 results of three methods whose entity is \"relational database\" are respectively 2, 2 and 3.\n",
    "\n",
    "About the query \"garbage collection\", the number of top 5 results of three methods whose entity is \"garbage collection\" are respectively 4, 3 and 3.\n",
    "\n",
    "About the query \"retrieval model\", the number of top 5 results of three methods whose entity is \"retrieval model\" are all 0. But the top 5 results of VSM-TF-IDF are the same as BM25. While from the perspective of meaning contained in the definition, i think the top 5 results of simple sums of TF-IDF are not so good as the VSM-TF-IDF and BM25.\n",
    "\n",
    "Hence, in this dataset, I prefer the BM25 method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Evaluation (10 points)\n",
    "Rather than just compare methods by pure observation, there are several metrics to evaluate the performance of an IR engine: Precision, Recall, MAP, NDCG, HitRate and so on. These all require a ground truth set of queries and documents with a notion of **relevance**. These ground truth judgments can be expensive to obtain, so we are cutting corners here and treating a flashcard's front and back as a \"relevant\" query-document pair.\n",
    "\n",
    "That is, if a document (definition) in your top-5 results is from the back of query's (entity's) flashcard, this document is regarded as relevant to the query (entity). This document is also called a hit in IR. Based on the ground-truth, you could calculate the metrics for the three ranking methods and provide the results like these:\n",
    "\n",
    "* metric: Precision@5\n",
    "* TF-IDF - score1\n",
    "* Vector Space Model with TF-IDF - score2\n",
    "* BM25 - score3\n",
    "\n",
    "You could pick any of the reasonable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "use Precision as the metric\n",
    "calculate by hand\n",
    "\n",
    "About the method TF-IDF:\n",
    "the precision for the query of \"relational database\" is 2/5\n",
    "the precision for the query of \"garbage collection\" is 4/5\n",
    "the precision for the query of \"retrieval model\" is 0/5\n",
    "So the average precision for the method TF-IDF is score1=(2/5+4/5+0/5)/3=0.4\n",
    "\n",
    "About the method Vector Space Model with TF-IDF:\n",
    "the precision for the query of \"relational database\" is 2/5\n",
    "the precision for the query of \"garbage collection\" is 3/5\n",
    "the precision for the query of \"retrieval model\" is 0/5\n",
    "So the average precision for the method Vector Space Model with TF-IDF is score2=(2/5+3/5+0/5)/3=0.33\n",
    "\n",
    "About the method BM25:\n",
    "the precision for the query of \"relational database\" is 3/5\n",
    "the precision for the query of \"garbage collection\" is 3/5\n",
    "the precision for the query of \"retrieval model\" is 0/5\n",
    "So the average precision for the method BM25 is score3=(3/5+3/5+0/5)/3=0.4\n",
    "\n",
    "So, score1=score3>score2\n",
    "\n",
    "Hence, just from the perspective of the precision score in the range of top-5 result, we may think the method TF-IDF and BM25 are better than\n",
    "Vector Space Model with TF-IDF. And the method TF-IDF is as good as BM25.\n",
    "\n",
    "But considering the meaning contained in the documents, we may think that BM25 is the best. \n",
    "As its result is more relevant than TF-IDF's result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** You should fill out your collaboration declarations here.**\n",
    "\n",
    "**Reminder:** You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by filling out the Collaboration Declarations at the bottom of this notebook.\n",
    "\n",
    "Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
