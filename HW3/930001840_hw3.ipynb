{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 3:   Recommender System Practice: Rating Prediction and Top-K Item Recommendation\n",
    "\n",
    "### 100 points [ 6% of your final grade]\n",
    "\n",
    "### Due: April 10, 2020\n",
    "\n",
    "*Goals of this homework:* Understand matrix factorization (MF) using explicit feedback and Bayesian Personalized Ranking (BPR) using implicit feedback for recommendation. Explore different methods for two real-world recommendation senarios: rating prediction and top-K item recommendation.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw3.ipynb`. For example, my homework submission would be something like `555001234_hw3.ipynb`. Submit this notebook via eCampus (look for the homework 3 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the total late days you have remaining).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Matrix Factorization for Rating Prediction (70 points total)\n",
    "\n",
    "In some platforms, such as MovieLens, users express their preference on items using explict feedback like ratings.\n",
    "\n",
    "In this part, you will implement matrix factorization to predict ratings on MovieLens data. After removing users who left less than 20 ratings and movies with less than 20 ratings, the provided dataset has only ~1,200 items and ~500 users. You can also check the title and genres of each movie in *movies_info.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Load the Data (5 points)\n",
    "\n",
    "Please download the dataset from Piazza. There are about 65,000 ratings in total. We split the rating data into two sets. You will train with 70% of the data (in *train_movie.csv*) and test on the remaining 30% of data (in *test_movie.csv*). Each of train and test files has lines having this format: UserID, MovieID, Rating. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set is: 541\n",
      "Number of movie in train set is: 1211\n",
      "Number of users in test set is: 541\n",
      "Number of movie in test set is: 1211\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep=',', header=None, skiprows=1)\n",
    "    data.columns = ['user','movie','rating']\n",
    "    return data\n",
    "\n",
    "train = load_data('train_movie.csv')\n",
    "test = load_data('test_movie.csv')\n",
    "\n",
    "number_users = len(train.user.unique())\n",
    "number_movies = len(train.movie.unique())\n",
    "print(\"Number of users in train set is:\" , number_users)\n",
    "print(\"Number of movie in train set is:\" , number_movies)\n",
    "\n",
    "number_users = len(test.user.unique())\n",
    "number_movies = len(test.movie.unique())\n",
    "print(\"Number of users in test set is:\" , number_users)\n",
    "print(\"Number of movie in test set is:\" , number_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: Matrix Factorization (40 points)\n",
    "\n",
    "In class, we introduced how matrix factorization works for recommendation. Now it is your term to implement it. There are different methods to obtain the latent factor matrices **P** and **Q**, like gradient descent, Alternating Least Squares (ALS), and so on. Pick one of them and implement your MF model. *You can refer to tutorials and resources online. Remember our **collaboration policy** and you need to inform us of the resources you refer to.* \n",
    "\n",
    "Please report MAE and RMSE of your MF model for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "\n",
    "data_matrix = train.pivot_table(index='movie', columns='user', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MF(dataMat, k, alpha, beta, maxIter):\n",
    "    for step in range(maxIter):\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if dataMat[i, j] > 0:\n",
    "                    error = dataMat[i, j]\n",
    "                    for r in range(k):\n",
    "                        error = error - p[i, r] * q[r, j]\n",
    "                    for r in range(k):\n",
    "                        p[i, r] = p[i, r] + alpha * (2 * error * q[r, j] - beta * p[i, r])\n",
    "                        q[r, j] = q[r, j] + alpha * (2 * error * p[i, r] - beta * q[r, j])\n",
    "        loss = 0.0\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if dataMat[i, j] > 0:\n",
    "                    error = 0.0\n",
    "                    for r in range(k):\n",
    "                        error = error + p[i, r] * q[r, j]\n",
    "                    loss = np.power((dataMat[i, j] - error), 2)\n",
    "                    for r in range(k):\n",
    "                        loss = loss + beta * (p[i, r] * p[i, r] + q[r, j] * q[r, j]) / 2\n",
    "        if loss < 0.001:\n",
    "            break\n",
    "    return p, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array(data_matrix)\n",
    "m, n = np.shape(data_matrix)\n",
    "k = 2\n",
    "\n",
    "p = np.random.rand(m, k)\n",
    "q = np.random.rand(k, n)\n",
    "\n",
    "\n",
    "P, Q = MF(matrix, k, 0.0003, 0.04, 100)\n",
    "R = np.dot(P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings():\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for index, row in test.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predicted_ratings.append(R[row[\"movie\"]][row[\"user\"]])\n",
    "    return predicted_ratings, actual_ratings\n",
    "\n",
    "predicted_ratings, actual_ratings = predict_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8873527538813737\n",
      "MAE = 0.6784261394149478\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def RMS(act, pred):\n",
    "    t = 0\n",
    "    for i in range(len(act)):\n",
    "        t = t + pow((act[i] - pred[i]),2)\n",
    "    t = t / len(act)\n",
    "    t = math.sqrt(t)\n",
    "    return t\n",
    "\n",
    "def MAE(act, pred):\n",
    "    t = 0\n",
    "    for i in range(len(act)):\n",
    "        t = t + abs((act[i] - pred[i]))\n",
    "    t = t / len(act)\n",
    "    return t\n",
    "\n",
    "print(\"RMSE =\", RMS(actual_ratings, predicted_ratings))\n",
    "print(\"MAE =\", MAE(actual_ratings, predicted_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method did you use to obtain **P** and **Q**? What are the advantages and disadvantages of the method you pick? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent and regularization to obtain P and Q.\n",
    "\n",
    "The advantage of gradient descent is that it requires no computation of Hessian matrix, making it fast per iteration.\n",
    "\n",
    "The disadvantage of gradient descent is that if learning rate is not set properly, it takes a lot of iterations to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1c: Improve MF (25 points)\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach. Hints: You may consider using the title or genres information, trying other algorithms, designing a hybrid system or considering a neighborhood like this paper [Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf). *You can do anything you like to improve MAE and RMSE.*\n",
    "\n",
    "You will get full marks for this part if you get better results than your MF results (of course we will also judge whether what you do here is reasonable or not). You will get partial marks for a reasonable effort even if you do not improve your MF results. Additionally, you will get 5 points as bonus if your model performs the best among the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "\n",
    "f = open(\"train_movie.csv\", encoding='UTF-8')\n",
    "f2 = open(\"train_movie_new.csv\", 'w', encoding='UTF-8')\n",
    "line = f.readline()\n",
    "while line:\n",
    "    if line != \"\\n\":\n",
    "        f2.write(line)\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "\n",
    "reader = surprise.Reader(sep=',', skip_lines=1)\n",
    "data = surprise.Dataset.load_from_file('train_movie_new.csv', reader)\n",
    "\n",
    "alg = surprise.SVDpp()\n",
    "output = alg.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8745614362635963\n",
      "MAE = 0.6720675480371989\n"
     ]
    }
   ],
   "source": [
    "def SVDpp():\n",
    "    for index, row in test.iterrows():\n",
    "        act_ratings.append(row[\"rating\"])\n",
    "        predict = alg.predict(str(row[\"user\"]), str(row[\"movie\"]))\n",
    "        pred_svdpp.append(predict.est)\n",
    "    return pred_svdpp, act_ratings\n",
    "\n",
    "act_ratings = []\n",
    "pred_svdpp = []\n",
    "pred_svdpp, act_svdpp = SVDpp()\n",
    "\n",
    "\n",
    "print(\"RMSE =\", RMS(act_svdpp, pred_svdpp))\n",
    "print(\"MAE =\", MAE(act_svdpp, pred_svdpp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain what you do to improve the recommendation in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SVD++ to improve the recommendation. Implement it by using the surprise package.\n",
    "\n",
    "The SVD++ algorithm, an extension of SVD taking into account implicit ratings. In the above case, the implicit ratings are the movie rating given by users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Bayesian Personalized Ranking (BPR) for Top-K Item Recommendation (30 points)\n",
    "\n",
    "Compared with rating prediction in part 1, a more popular scenario recently is personalized top-K item ranking for each user based on the user's implicit feedback. Examples include ranking videos on YouTube and ranking products on Aamzon. In practice, users tend to provide implicit feedback (e.g., the user clicked a product URL on Amazon or played a video on YouTube) rather than explicit feedback (e.g., ratings or reviews) in most cases.\n",
    "\n",
    "In this part, you will experiment with Bayesian Personalized Ranking (BPR) to rank items on a [Spotify Playlist Recommendation Dataset](http://people.tamu.edu/~yunhe/pubs/AttListCIKM2019.pdf). If a user ever followed a playlist, this interaction is treated as an implicit feedback. In our sampled dataset, there are ~10,000 users and ~7,000 playlists.\n",
    "\n",
    "BPR can generate scores of items for each user. You should rank all items based on the scores for each user and evaluate the ranking performance.\n",
    "\n",
    "For example, if user 0 has two interacted playlists 23, 78 in test.txt. If the top-10 playlists for user 0 returned by BPR is [12,45,78,34,23,90,134,33,46,9], then the precision@10 for user 0 is 0.2 because the two playlists in test.txt are recommended in top-10: 2/10=0.2. Please report NDCG@10 in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Please download the dataset from Piazza. There are about 90,000 interactions in total, which are split into training.txt, validation.txt and text.txt. You will train on train.txt, tune hyperparameters on validation.txt and report final result on test.txt in terms of NDCG@10. \n",
    "\n",
    "Each of the train and test files has lines having this format: UserID, PlaylistID, 1.0. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set is: 10183\n",
      "Number of playlists in train set is: 7787\n",
      "Number of users in test set is: 5846\n",
      "Number of playlists in test set is: 3604\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# playlists and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', header=None, skiprows=1)\n",
    "    data.columns = ['UserID','PlaylistID','Value']\n",
    "    return data\n",
    "\n",
    "train=load_data('train.txt')\n",
    "test=load_data('test.txt')\n",
    "\n",
    "number_users = len(train.UserID.unique())\n",
    "number_movies = len(train.PlaylistID.unique())\n",
    "print(\"Number of users in train set is:\" , number_users)\n",
    "print(\"Number of playlists in train set is:\" , number_movies)\n",
    "\n",
    "number_users = len(test.UserID.unique())\n",
    "number_movies = len(test.PlaylistID.unique())\n",
    "print(\"Number of users in test set is:\" , number_users)\n",
    "print(\"Number of playlists in test set is:\" , number_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR by Using Package\n",
    "\n",
    "Compared with MF, BPR is more complicated to implement. In this part, you can use a BPR package to experiment with top-K item recommendation. Some good packages include https://github.com/benfred/implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to call other BPR packages for top-K recommendation.\n",
    "# Report average NDCG@10 for all users on test.txt\n",
    "\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "f = open(\"train.txt\", encoding='UTF-8')\n",
    "line = f.readline()\n",
    "UserID = []\n",
    "PlaylistID = []\n",
    "while line:\n",
    "    line_list = line.split(\"\\t\")\n",
    "    UserID.append(int(line_list[0]))\n",
    "    PlaylistID.append(int(line_list[1]))\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "\n",
    "data_coo_matrix = coo_matrix(([1.0] * len(UserID), (PlaylistID, UserID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6387dcd75a043269fae4e39ef8ef614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import implicit\n",
    "model = implicit.bpr.BayesianPersonalizedRanking()\n",
    "model.fit(data_coo_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train.txt\", encoding='UTF-8')\n",
    "line = f.readline()\n",
    "UserID = []\n",
    "PlaylistID = []\n",
    "while line:\n",
    "    line_list = line.split(\"\\t\")\n",
    "    UserID.append(int(line_list[0]))\n",
    "    PlaylistID.append(int(line_list[1]))\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "\n",
    "data_csr_matrix_train = coo_matrix(([1.0] * len(UserID), (UserID, PlaylistID))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test.txt\", encoding='UTF-8')\n",
    "line = f.readline()\n",
    "UserID = []\n",
    "PlaylistID = []\n",
    "while line:\n",
    "    line_list = line.split(\"\\t\")\n",
    "    UserID.append(int(line_list[0]))\n",
    "    PlaylistID.append(int(line_list[1]))\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "\n",
    "data_csr_matrix_test = coo_matrix(([1.0] * len(UserID), (UserID, PlaylistID))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86438e7121244399a00399a5fbcb0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10182.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@10 = 0.09944746866128336\n"
     ]
    }
   ],
   "source": [
    "from implicit.evaluation import ndcg_at_k\n",
    "ndcg = ndcg_at_k(model, data_csr_matrix_train, data_csr_matrix_test, K=10)\n",
    "print(\"NDCG@10 =\", ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
