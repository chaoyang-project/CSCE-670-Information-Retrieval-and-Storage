{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 2:  PageRank + Learning to Rank\n",
    "\n",
    "### 100 points [10% of your final grade]\n",
    "\n",
    "### Due: March 5, 2020 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will explore real-world challenges of building a graph (in this case, from tweets), implement and test the classic PageRank algortihm over this graph. In addition, you will apply learning to rank to a real-world dataset and report the performance in terms of NDCG.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PageRank (60 points)\n",
    "In this assignment, we're going to adapt the classic PageRank approach to allow us to find not the most authoritative web pages, but rather to find significant Twitter users. \n",
    "\n",
    "\n",
    "## Part 1.1: A re-Tweet Graph (20 points)\n",
    "\n",
    "So, instead of viewing the world as web pages with hyperlinks (where pages = nodes, hyperlinks = edges), we're going to construct a graph of Twitter users and their retweets of other Twitter users (so user = node, retweet of another user = edge). Over this Twitter-user graph, we can apply the PageRank approach to order the users. The main idea is that a user who is retweeted by other users is more \"impactful\". \n",
    "\n",
    "Here is a toy example. Suppose you are given the following four retweets:\n",
    "\n",
    "* **userID**: diane, **text**: \"RT \", **sourceID**: bob\n",
    "* **userID**: charlie, **text**: \"RT Welcome\", **sourceID**: alice\n",
    "* **userID**: bob, **text**: \"RT Hi \", **sourceID**: diane\n",
    "* **userID**: alice, **text**: \"RT Howdy!\", **sourceID**: parisa\n",
    "\n",
    "There are four short tweets retweeted by four users. The retweet between users form a directed graph with five nodes and four edges. E.g., the \"diane\" node has a directed edge to the \"bob\" node.\n",
    "\n",
    "You should build a graph by parsing the tweets in the file we provide called *PageRank.json*.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* You may see some weird characters in the content of tweets, just ignore them. \n",
    "* The edges are binary and directed. If Bob retweets Alice once, in 10 tweets, or 10 times in one tweet, there is an edge from Bob to Alice, but there is not an edge from Alice to Bob.\n",
    "* If a user retweets herself, ignore it.\n",
    "* Correctly parsing screen_name in a tweet is error-prone. Use the id of the user (this is the user who is re-tweeting) and the id of the user in the retweeted_status field (this is the user who is being re-tweeted; that is, this user created the original tweet).\n",
    "* Later you will need to implement the PageRank algorithm on the graph you build here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here define your function for building the graph by parsing \n",
    "# the input file of tweets\n",
    "# Insert as many cells as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call your function to print out the size of the graph, \n",
    "# i.e., the number of nodes and edges\n",
    "# How you maintain the graph is totaly up to you\n",
    "# However, if you encounter any memory issues, we recommend you \n",
    "#write the graph into a file, and load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes:  1003\n",
      "The number of edges:  6177\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "edges = 0\n",
    "graph = defaultdict(dict)\n",
    "\n",
    "\n",
    "def addEdge(userID, sourceID):\n",
    "    if sourceID in graph[userID]:\n",
    "        graph[userID][sourceID] = 1\n",
    "    else:\n",
    "        graph[userID][sourceID] = 1\n",
    "    if sourceID not in graph:\n",
    "        graph[sourceID][userID] = 0 \n",
    "        \n",
    "\n",
    "data_lines = open('HITS.json', encoding='UTF-8')\n",
    "for line in data_lines:\n",
    "    data = json.loads(line)\n",
    "    userID = data['user']['id']\n",
    "    sourceID = data['retweeted_status']['user']['id']\n",
    "    if userID == sourceID:\n",
    "        continue\n",
    "    addEdge(userID, sourceID)\n",
    "data_lines.close()\n",
    "    \n",
    "    \n",
    "def key_id(keys_sorted):\n",
    "    keys_id_dict = {}\n",
    "    j = 0\n",
    "    for i in keys_sorted:\n",
    "        keys_id_dict[i] = j\n",
    "        j += 1\n",
    "    return keys_id_dict\n",
    "\n",
    "\n",
    "# graph -> matrix\n",
    "def get_matrix(graph):\n",
    "    keys_sorted = sorted(graph.keys())\n",
    "    size = len(keys_sorted)\n",
    "    M = [[0] * size for i in range(size)]\n",
    "    keys_id_dict = key_id(keys_sorted)\n",
    "    for k1 in keys_sorted:\n",
    "        for k2 in keys_sorted:\n",
    "            if k1 == k2:\n",
    "                M[keys_id_dict[k1]][keys_id_dict[k2]] = 0\n",
    "            try:\n",
    "                M[keys_id_dict[k1]][keys_id_dict[k2]] = graph[k1][k2]\n",
    "            except:\n",
    "                M[keys_id_dict[k1]][keys_id_dict[k2]] = 0\n",
    "\n",
    "    return M, keys_id_dict\n",
    "\n",
    "\n",
    "M, keys_id_dict = get_matrix(graph)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "b = np.array(M,dtype = float)\n",
    "print(\"The number of nodes: \", b.shape[0])\n",
    "# print(b.shape[1])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(b.shape[0]):\n",
    "    for j in range(b.shape[1]):\n",
    "        if M[i][j] == 1:\n",
    "            count += 1\n",
    "print(\"The number of edges: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not check the correctness of your graph. However, this will affect the PageRank results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: PageRank Implementation (30 points)\n",
    "\n",
    "Your program will return the top 10 users with highest PageRank scores. The **output** should be like:\n",
    "\n",
    "* user1 - score1\n",
    "* user2 - score2\n",
    "* ...\n",
    "* user10 - score10\n",
    "\n",
    "You should follow these **rules**:\n",
    "\n",
    "* Assume all nodes start out with equal probability.\n",
    "* The probability of the random surfer teleporting is 0.1 (that is, the damping factor is 0.9).\n",
    "* If a user is never retweeted and does not retweet anyone, their PageRank scores should be zero. Do not include the user in the calculation.\n",
    "* It is up to you to decide when to terminate the PageRank calculation.\n",
    "* There are PageRank implementations out there on the web. Remember, your code should be **your own**.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "* If you're using the matrix style approach, you should use [numpy.matrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html).\n",
    "* Scipy is built on top of Numpy and has support for sparse matrices. You most likely will not need to use Scipy unless you'd like to try out their sparse matrices.\n",
    "* If you choose to use Numpy (and Scipy), please make sure your Anaconda environment include their latest versions.\n",
    "* Test your parsing and PageRank calculations using a handful of tweets, before moving on to the entire file we provide.\n",
    "* We will evaluate the user ranks you provide as well as the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the termination condition in your PageRank implementation? Describe it below:\n",
    "\n",
    "*ADD YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here add your code to implement a function called PageRanker\n",
    "# Insert as many cells as you want\n",
    "\n",
    "# def PageRanker(...):\n",
    "#    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(b)\n",
    "\n",
    "\n",
    "def graphMove(a):\n",
    "    c = np.zeros((1003, 1003),dtype = float)\n",
    "    for i in range(1003):\n",
    "        for j in range(1003):\n",
    "            if b[j].sum()==0:\n",
    "                c[i][j] = 0\n",
    "            else:\n",
    "                c[i][j] = a[i][j] / (b[j].sum())\n",
    "    return c\n",
    "\n",
    "\n",
    "def firstPr():\n",
    "    pr = np.zeros((1003, 1), dtype=float)\n",
    "    for i in range(1003):\n",
    "        pr[i] = float(1)/1003\n",
    "    return pr\n",
    "\n",
    "\n",
    "mm = np.zeros((1003, 1003), dtype=float)\n",
    "for i in range (1003):\n",
    "    for j in range (1003):\n",
    "        mm[i][j] = 1/1003\n",
    "        \n",
    "def PageRanker(p,m,pr):\n",
    "    T = p * m + (1 - p) * mm\n",
    "    pr = np.transpose(pr)\n",
    "    for i in range(100):\n",
    "        pr = np.dot(pr, T)\n",
    "        pr /= np.sum(pr)\n",
    "    return pr\n",
    "\n",
    "M = graphMove(a)\n",
    "M = np.transpose(M)\n",
    "pr = firstPr()\n",
    "p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's call your function on the graph you've built. Output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183906148 \t 0.03690928484050658\n",
      "2598548166 \t 0.023700823719667377\n",
      "3019659587 \t 0.022420060162606687\n",
      "3077695572 \t 0.022046237452155906\n",
      "3154266823 \t 0.01990739513690696\n",
      "3042570996 \t 0.01977500924476432\n",
      "3068694151 \t 0.01954808142465658\n",
      "3264645911 \t 0.017964127148931492\n",
      "3082766914 \t 0.017457986143068278\n",
      "571198546 \t 0.016351985674481474\n"
     ]
    }
   ],
   "source": [
    "PR = PageRanker(p,M,pr)\n",
    "\n",
    "\n",
    "# print\n",
    "def key_id(keys_sorted):\n",
    "    keys_id_dict = {}\n",
    "    j = 0\n",
    "    for i in keys_sorted:\n",
    "        keys_id_dict[i] = j\n",
    "        j += 1\n",
    "    return keys_id_dict\n",
    "\n",
    "\n",
    "pr_dict = {}\n",
    "for i in range(1003):\n",
    "    pr_dict[i] = PR[0][i]\n",
    "\n",
    "    \n",
    "values = sorted(pr_dict.items(), key=lambda d: d[1], reverse=True)  \n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    for k,v in keys_id_dict.items():\n",
    "        if v == values[i][0]:\n",
    "            print(k, '\\t', values[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Improving PageRank (10 points)\n",
    "In the many years since PageRank was introduced, there have been many improvements and extensions. For this part, you should experiment with one such improvement and then compare the results you get with the original results in Part 1.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3042570996 \t 0.058613667318172254\n",
      "2860872854 \t 0.04766331262265769\n",
      "1183906148 \t 0.0338283838786767\n",
      "3142161801 \t 0.026258939442898775\n",
      "610166901 \t 0.025730931339386323\n",
      "3154266823 \t 0.02567377376546478\n",
      "2598548166 \t 0.023083933063185732\n",
      "3198584744 \t 0.021613601439217874\n",
      "3156878078 \t 0.020016610074063287\n",
      "3169039209 \t 0.019856262064168316\n"
     ]
    }
   ],
   "source": [
    "graph = defaultdict(dict)\n",
    "\n",
    "\n",
    "def addEdge(userID, sourceID):\n",
    "    if sourceID in graph[userID]:\n",
    "        graph[userID][sourceID] += 1\n",
    "    else:\n",
    "        graph[userID][sourceID] = 1\n",
    "    if sourceID not in graph:\n",
    "        graph[sourceID][userID] = 0\n",
    "        \n",
    "\n",
    "data_lines = open('HITS.json', encoding='UTF-8')\n",
    "for line in data_lines:\n",
    "    data = json.loads(line)\n",
    "    userID = data['user']['id']\n",
    "    sourceID = data['retweeted_status']['user']['id']\n",
    "    if userID == sourceID:\n",
    "        continue\n",
    "    addEdge(userID, sourceID)\n",
    "data_lines.close()\n",
    "    \n",
    "    \n",
    "M, keys_id_dict = get_matrix(graph)\n",
    "\n",
    "\n",
    "b = np.array(M,dtype = float)\n",
    "a = np.transpose(b)\n",
    "M = graphMove(a)\n",
    "M = np.transpose(M)\n",
    "pr = firstPr()\n",
    "p = 0.9\n",
    "\n",
    "\n",
    "PR = PageRanker(p,M,pr)\n",
    "\n",
    "\n",
    "pr_dict = {}\n",
    "for i in range(1003):\n",
    "    pr_dict[i] = PR[0][i]\n",
    "\n",
    "    \n",
    "values = sorted(pr_dict.items(), key=lambda d: d[1], reverse=True)  \n",
    "   \n",
    "    \n",
    "for i in range(10):\n",
    "    for k,v in keys_id_dict.items():\n",
    "        if v == values[i][0]:\n",
    "            print(k, '\\t', values[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plus be sure to describe your extension (what is it? \n",
    "The edges are not binary any more. Instead, the value of the edge in matrix is the number of edge from userID to sourceID.\n",
    "# why did you choose it?) and your comparison to Part 1.2\n",
    "Because if A retweets B more times, it means B is of more value or importance.\n",
    "So by this method, the top10 results are more reasonable than Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning to Rank (40 points)\n",
    "\n",
    "For this part, we're going to play with some Microsoft LETOR data that has query-document relevance judgments. Let's see how learning to rank works in practice. \n",
    "\n",
    "First, you will need to download the MQ2008.zip file from the Resources tab on Piazza. This is data from the [Microsoft Research IR Group](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/).\n",
    "\n",
    "The data includes 15,211 rows. Each row is a query-document pair. The first column is a relevance label of this pair (0,1 or 2--> the higher value the more related), the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. A query-document pair is represented by a 46-dimensional feature vector. Features are a numeric value describing a document and query such as TFIDF, BM25, Page Rank, .... You can find compelete description of features from [here](https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf).\n",
    "\n",
    "The good news for you is the dataset is ready for analysis: It has already been split into 5 folds (see the five folders called Fold1, ..., Fold5).\n",
    "\n",
    "\n",
    "## Part 2.1: Build Point-wise Learning to Rank  (20 points)\n",
    "First, you should build a point-wise Learning to Rank framework. \n",
    "1. You could train a binary classification model like SVM or logistic regression on the train file. In this case, 0 is treated as negative (irrelevant) sample and 1, 2 are treated as positive (relevant) sample.\n",
    "2. You apply the already trained model to predict the scores for documents on test file.\n",
    "3. Order the documents based on the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add your results and discussion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainDataSet(path):\n",
    "    f = open(path, encoding='UTF-8')\n",
    "    line = f.readline()\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    while line:\n",
    "        each_line_list = []\n",
    "        j = 2\n",
    "        line_list = line.split()\n",
    "        for i in range(46):\n",
    "            each_line_list.append(float(line_list[j].split(\":\")[1]))\n",
    "            j = j + 1\n",
    "        x_train.append(each_line_list)\n",
    "        y_train.append(int(line_list[0]))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] != 0:\n",
    "            y_train[i] = 1\n",
    "    return x_train, y_train\n",
    "            \n",
    "            \n",
    "def createValiDataSet(path):\n",
    "    f = open(path, encoding='UTF-8')\n",
    "    line = f.readline()\n",
    "    x_vali = []\n",
    "    y_vali = []\n",
    "    while line:\n",
    "        each_line_list = []\n",
    "        j = 2\n",
    "        line_list = line.split()\n",
    "        for i in range(46):\n",
    "            each_line_list.append(float(line_list[j].split(\":\")[1]))\n",
    "            j = j + 1\n",
    "        x_vali.append(each_line_list)\n",
    "        y_vali.append(int(line_list[0]))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    x_vali = np.array(x_vali)\n",
    "    y_vali = np.array(y_vali)\n",
    "    for i in range(len(y_vali)):\n",
    "        if y_vali[i] != 0:\n",
    "            y_vali[i] = 1\n",
    "    return x_vali, y_vali\n",
    "    \n",
    "\n",
    "def createTestDataSet(path):\n",
    "    f = open(path, encoding='UTF-8')\n",
    "    line = f.readline()\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    qid_test = []\n",
    "    docid_test = []\n",
    "    rel_test = []\n",
    "    while line:\n",
    "        each_line_list = []\n",
    "        j = 2\n",
    "        line_list = line.split()\n",
    "        for i in range(46):\n",
    "            each_line_list.append(float(line_list[j].split(\":\")[1]))\n",
    "            j = j + 1\n",
    "        x_test.append(each_line_list)\n",
    "        y_test.append(int(line_list[0]))\n",
    "        rel_test.append(int(line_list[0]))\n",
    "        qid_test.append(int(line_list[1].split(\":\")[1]))\n",
    "        docid_test.append(line_list[50].split()[0])\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    rel_test = np.array(rel_test)\n",
    "    # qid_test = np.array(qid_test)\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != 0:\n",
    "            y_test[i] = 1\n",
    "    return x_test, y_test, rel_test, qid_test, docid_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_docid_print():\n",
    "    # prepare for print\n",
    "    c = clf.predict_log_proba (x_test)\n",
    "    \n",
    "    score_list = []\n",
    "    for i in range(len(c)):\n",
    "        score_list.append(c[i][1])\n",
    "    \n",
    "    qid_docid_score_dict = {}\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        qid_docid_list = [docid_test[i], score_list[i]]\n",
    "        if qid_test[i] not in qid_docid_score_dict:\n",
    "            qid_docid_score_dict[qid_test[i]] = [qid_docid_list]\n",
    "        else:\n",
    "            qid_docid_score_dict[qid_test[i]].append(qid_docid_list)\n",
    "\n",
    "    # print\n",
    "    for key, value in qid_docid_score_dict.items():\n",
    "        value.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"qid: \", key)\n",
    "        for i in range(len(value)):\n",
    "            print(value[i][0], '\\t', value[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold1\\\\train.txt\")\n",
    "x_vali, y_vali = createValiDataSet(\"MQ2008\\\\MQ2008\\\\Fold1\\\\vali.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold1\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_vali)\n",
    "print(\"Accuracy on {} vali file is {}\".format(\"Fold1\", metrics.accuracy_score(y_vali, predictions)))\n",
    "# apply the model on test file and print\n",
    "print(\"For each qid in Fold1 test file, sort the documents:\")\n",
    "sorted_docid_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold2\\\\train.txt\")\n",
    "x_vali, y_vali = createValiDataSet(\"MQ2008\\\\MQ2008\\\\Fold2\\\\vali.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold2\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_vali)\n",
    "print(\"Accuracy on {} vali file is {}\".format(\"Fold2\", metrics.accuracy_score(y_vali, predictions)))\n",
    "# apply the model on test file and print\n",
    "print(\"For each qid in Fold2 test file, sort the documents:\")\n",
    "sorted_docid_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold3\\\\train.txt\")\n",
    "x_vali, y_vali = createValiDataSet(\"MQ2008\\\\MQ2008\\\\Fold3\\\\vali.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold3\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_vali)\n",
    "print(\"Accuracy on {} vali file is {}\".format(\"Fold3\", metrics.accuracy_score(y_vali, predictions)))\n",
    "# apply the model on test file and print\n",
    "print(\"For each qid in Fold3 test file, sort the documents:\")\n",
    "sorted_docid_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold4\\\\train.txt\")\n",
    "x_vali, y_vali = createValiDataSet(\"MQ2008\\\\MQ2008\\\\Fold4\\\\vali.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold4\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_vali)\n",
    "print(\"Accuracy on {} vali file is {}\".format(\"Fold4\", metrics.accuracy_score(y_vali, predictions)))\n",
    "# apply the model on test file and print\n",
    "print(\"For each qid in Fold4 test file, sort the documents:\")\n",
    "sorted_docid_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold5\\\\train.txt\")\n",
    "x_vali, y_vali = createValiDataSet(\"MQ2008\\\\MQ2008\\\\Fold5\\\\vali.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold5\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_vali)\n",
    "print(\"Accuracy on {} vali file is {}\".format(\"Fold5\", metrics.accuracy_score(y_vali, predictions)))\n",
    "# apply the model on test file and print\n",
    "print(\"For each qid in Fold5 test file, sort the documents:\")\n",
    "sorted_docid_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: NDCG (20 points)\n",
    "\n",
    "Based on your prediction file (results could be ranked by scores in the prediction file) and ground-truth (i.e., 0,1,2) in the test file, calculate NDCG for each query. Report average NDCG for all queries in the five-fold cross validation.\n",
    "\n",
    "For NDCG, please bulid your own function rather then using any package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def NDCG():\n",
    "    c = clf.predict_log_proba (x_test)\n",
    "    score_list = []\n",
    "    for i in range(len(c)):\n",
    "        score_list.append(c[i][1])\n",
    "    qid_docid_score_dict = {}\n",
    "    for i in range(len(c)):\n",
    "        qid_docid_list = [docid_test[i], score_list[i], rel_test[i]]\n",
    "        if qid_test[i] not in qid_docid_score_dict:\n",
    "            qid_docid_score_dict[qid_test[i]] = [qid_docid_list]\n",
    "        else:\n",
    "            qid_docid_score_dict[qid_test[i]].append(qid_docid_list)\n",
    "    sum_NDCG = 0\n",
    "    for key, value in qid_docid_score_dict.items():\n",
    "        value.sort(key=lambda x: x[1], reverse=True)\n",
    "    #     print(key)\n",
    "    #     for i in range(len(value)):\n",
    "    #         print(value[i][0], '\\t', value[i][1], '\\t', value[i][2])\n",
    "        DCG = 0\n",
    "        if len(value) >= 10:\n",
    "            for j in range(10):\n",
    "                DCG += value[j][2]/np.log2(j+2)\n",
    "        else:\n",
    "            for j in range(len(value)):\n",
    "                DCG += value[j][2]/np.log2(j+2)\n",
    "        # IDCG\n",
    "        value.sort(key=lambda x: x[2], reverse=True)\n",
    "        IDCG = 0\n",
    "        for j in range(len(value)):\n",
    "                IDCG += value[j][2]/np.log2(j+2)\n",
    "        # NDCG\n",
    "        if IDCG == 0:\n",
    "            NDCG = 0\n",
    "        else:\n",
    "            NDCG = DCG/IDCG\n",
    "        sum_NDCG += NDCG\n",
    "        \n",
    "    average_NDCG = sum_NDCG/len(qid_docid_score_dict)\n",
    "    return average_NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1_average_NDCG:  0.431112388577871\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold1\\\\train.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold1\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "Fold1_average_NDCG = NDCG()\n",
    "print(\"Fold1_average_NDCG: \", Fold1_average_NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold2_average_NDCG:  0.433529582537733\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold2\\\\train.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold2\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "Fold2_average_NDCG = NDCG()\n",
    "print(\"Fold2_average_NDCG: \", Fold2_average_NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold3_average_NDCG:  0.4538636734023412\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold3\\\\train.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold3\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "Fold3_average_NDCG = NDCG()\n",
    "print(\"Fold3_average_NDCG: \", Fold3_average_NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold4_average_NDCG:  0.5022472279652941\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold4\\\\train.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold4\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "Fold4_average_NDCG = NDCG()\n",
    "print(\"Fold4_average_NDCG: \", Fold4_average_NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold5_average_NDCG:  0.5091206325695353\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = createTrainDataSet(\"MQ2008\\\\MQ2008\\\\Fold5\\\\train.txt\")\n",
    "x_test, y_test, rel_test, qid_test, docid_test = createTestDataSet(\"MQ2008\\\\MQ2008\\\\Fold5\\\\test.txt\")\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True)\n",
    "clf.fit(x_train, y_train)\n",
    "Fold5_average_NDCG = NDCG()\n",
    "print(\"Fold5_average_NDCG: \", Fold5_average_NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG in the whole dataset:  0.4659747010105549\n"
     ]
    }
   ],
   "source": [
    "sum_NDCG = Fold1_average_NDCG + Fold2_average_NDCG + Fold3_average_NDCG + Fold4_average_NDCG + Fold5_average_NDCG\n",
    "NDCG = sum_NDCG / 5\n",
    "print(\"NDCG in the whole dataset: \", NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
